# RAG Chatbot - Production-Style Learning Project

A comprehensive Retrieval-Augmented Generation (RAG) chatbot built with LangChain, designed as a learning project for understanding LLMs and RAG systems end-to-end.

## ğŸ—ï¸ Architecture

### Tech Stack

**Backend:**
- **FastAPI** - Modern, fast Python web framework (similar to Spring Boot in philosophy)
- **LangChain** - LLM orchestration and RAG pipeline
- **LangChain Community** - Document loaders and integrations

**Embeddings & LLM:**
- **Hugging Face** (default, free tier) - Sentence transformers for embeddings, open-source LLMs for generation
  - Embeddings: `sentence-transformers/all-MiniLM-L6-v2` (lightweight, fast)
  - LLM: `microsoft/DialoGPT-medium` or `google/flan-t5-base` (CPU-friendly)
  - Can run locally without API costs

**Vector Database:**
- **Chroma** (MVP) - Lightweight, in-memory vector store
- **Qdrant** (Production) - Scalable, persistent vector database

**Frontend:**
- **Streamlit** (MVP) - Rapid prototyping
- **React/Next.js** (Future) - Production frontend

**Additional:**
- **PyPDF2** / **pypdf** - PDF document processing
- **python-dotenv** - Environment configuration
- **pytest** - Testing framework

## ğŸ“‹ Project Roadmap

### Phase 1: MVP (Week 1-2)
- [x] Project setup and architecture
- [ ] PDF document ingestion pipeline
- [ ] Text chunking and embedding generation
- [ ] Vector store integration
- [ ] Basic RAG query endpoint
- [ ] Simple Streamlit UI

### Phase 2: Core Features (Week 3-4)
- [ ] Multi-document support
- [ ] Chat history and conversation context
- [ ] Source citation and retrieval metadata
- [ ] Document management (upload/delete)
- [ ] Error handling and validation

### Phase 3: Enhancement (Week 5-6)
- [ ] Advanced chunking strategies (semantic, recursive)
- [ ] Query optimization (query rewriting, reranking)
- [ ] Prompt templates and system prompts
- [ ] Response streaming
- [ ] Performance monitoring

### Phase 4: Testing & Quality (Week 7)
- [ ] Unit tests for core components
- [ ] Integration tests for API endpoints
- [ ] RAG evaluation metrics (retrieval accuracy, answer quality)
- [ ] Load testing

### Phase 5: Deployment (Week 8)
- [ ] Docker containerization
- [ ] Environment configuration
- [ ] Cloud deployment guide (AWS/GCP/Azure)
- [ ] CI/CD pipeline setup

## ğŸš€ Quick Start

### Prerequisites
- Python 3.9+
- Hugging Face account (free) - Get token from https://huggingface.co/settings/tokens
- 4GB+ RAM recommended (for local models)
- Optional: GPU for faster inference (CUDA-compatible)

### Installation

```bash
# Clone repository
git clone <repo-url>
cd rag-chatbot

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your Hugging Face token (optional, but recommended for private models)
```

### Run Application

```bash
# Start FastAPI backend
uvicorn app.main:app --reload

# Start Streamlit frontend (in another terminal)
streamlit run app/frontend/streamlit_app.py
```

## ğŸ“ Project Structure

```
rag-chatbot/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI application
â”‚   â”œâ”€â”€ config.py               # Configuration management
â”‚   â”œâ”€â”€ models/                 # Data models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document.py
â”‚   â”‚   â””â”€â”€ query.py
â”‚   â”œâ”€â”€ services/               # Business logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document_service.py # Document ingestion
â”‚   â”‚   â”œâ”€â”€ embedding_service.py # Embedding generation
â”‚   â”‚   â”œâ”€â”€ vector_store.py     # Vector DB operations
â”‚   â”‚   â””â”€â”€ rag_service.py      # RAG pipeline
â”‚   â”œâ”€â”€ api/                    # API routes
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ documents.py
â”‚   â”‚   â””â”€â”€ chat.py
â”‚   â””â”€â”€ frontend/               # Frontend interfaces
â”‚       â””â”€â”€ streamlit_app.py
â”œâ”€â”€ tests/                      # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_document_service.py
â”‚   â”œâ”€â”€ test_rag_service.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ data/                       # Document storage
â”‚   â””â”€â”€ uploads/
â”œâ”€â”€ .env.example                # Environment template
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ Dockerfile                  # Container definition
â””â”€â”€ README.md                   # This file
```

## ğŸ¯ Key Concepts & Best Practices

### 1. Document Processing Pipeline
- **Loading**: PDF â†’ Text extraction
- **Chunking**: Text â†’ Semantic chunks (overlap for context)
- **Embedding**: Chunks â†’ Vector representations
- **Storage**: Vectors â†’ Vector database with metadata

### 2. RAG Query Flow
1. User query â†’ Embedding
2. Similarity search in vector store
3. Retrieve top-k relevant chunks
4. Construct prompt with context
5. LLM generation with retrieved context
6. Return response with citations

### 3. Prompt Engineering
- **System Prompt**: Define assistant role and behavior
- **Context Injection**: Include retrieved documents
- **Query Formatting**: Structure for clarity
- **Few-shot Examples**: Guide model behavior

### 4. Cost Control
- Use appropriate model tiers (GPT-3.5-turbo vs GPT-4)
- Cache embeddings (don't re-embed unchanged documents)
- Limit context window size
- Monitor token usage

### 5. Evaluation Metrics
- **Retrieval Accuracy**: Are relevant chunks retrieved?
- **Answer Quality**: Is the answer correct and complete?
- **Latency**: Response time
- **Cost per Query**: Token usage tracking

## ğŸ“š Learning Resources

- [LangChain Documentation](https://python.langchain.com/)
- [RAG Paper](https://arxiv.org/abs/2005.11401)
- [Vector Database Guide](https://www.pinecone.io/learn/vector-database/)

## ğŸ¤ Contributing

This is a learning project. Feel free to experiment, modify, and extend!

## ğŸ“„ License

MIT License
